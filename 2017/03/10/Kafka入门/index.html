<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Kafka 入门 | WolfcsTech</title><link rel="stylesheet" type="text/css" href="//fonts.css.network/css?family=Source+Code+Pro"><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.2.0"><link rel="stylesheet" type="text/css" href="/css/highlight.css?v=1.2.0"><link rel="Shortcut Icon" href="/favicon.ico"><link rel="bookmark" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Kafka 入门</h1><a id="logo" href="/.">WolfcsTech</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div id="layout" class="layout-g"><div class="layout-l"><div class="content_container"><div class="post"><h1 class="post-title">Kafka 入门</h1><div class="post-meta">Mar 10, 2017<span> | </span><span class="category"><a href="/categories/Java开发/">Java开发</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><a data-disqus-identifier="2017/03/10/Kafka入门/" href="/2017/03/10/Kafka入门/#disqus_thread" class="disqus-comment-count"></a><div class="post-content"><p>这份指南假设你是新手，且还没有 Kafka 和 ZooKeeper 数据。由于 Kafka 针对基于 Unix 的平台和 Windows 平台的终端脚本是不同的，因而，在 Windows 平台下，请使用 <code>bin\windows</code> 下的来代替 <code>bin/</code> 下的，并将脚本的扩展名修改为 <code>.bat</code>。<br> <a id="more"></a></p>
<h1 id="第-1-步：下载代码。"><a href="#第-1-步：下载代码。" class="headerlink" title="第 1 步：下载代码。"></a>第 1 步：下载代码。</h1><p><a href="https://www.apache.org/dyn/closer.cgi?path=/kafka/0.10.2.0/kafka_2.11-0.10.2.0.tgz" target="_blank" rel="external">下载</a> 0.10.20.0 发布版并 un-tar 它。<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ tar -xzf kafka_2<span class="number">.11</span><span class="number">-0.10</span><span class="number">.2</span><span class="number">.0</span>.tgz</div><div class="line">$ cd kafka_2<span class="number">.11</span><span class="number">-0.10</span><span class="number">.2</span><span class="number">.0</span></div></pre></td></tr></table></figure></p>
<h1 id="第-2-步，启动服务器"><a href="#第-2-步，启动服务器" class="headerlink" title="第 2 步，启动服务器"></a>第 2 步，启动服务器</h1><p>Kafka 使用 ZooKeeper，因而如果你还没有启动它的话，你需要先启动一个 ZooKeeper 服务器。你可以使用包装了 kafka 的脚本来得到一个快速而干净的单节点 ZooKeeper 实例。<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">$ bin/zookeeper-server-start<span class="selector-class">.sh</span> config/zookeeper<span class="selector-class">.properties</span></div><div class="line">[<span class="number">2017</span>-<span class="number">03</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">33</span>:<span class="number">43</span>,<span class="number">848</span>] INFO Reading configuration from: config/zookeeper<span class="selector-class">.properties</span> (org<span class="selector-class">.apache</span><span class="selector-class">.zookeeper</span><span class="selector-class">.server</span><span class="selector-class">.quorum</span><span class="selector-class">.QuorumPeerConfig</span>)</div><div class="line">[<span class="number">2017</span>-<span class="number">03</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">33</span>:<span class="number">43</span>,<span class="number">889</span>] INFO autopurge<span class="selector-class">.snapRetainCount</span> set to <span class="number">3</span> (org<span class="selector-class">.apache</span><span class="selector-class">.zookeeper</span><span class="selector-class">.server</span><span class="selector-class">.DatadirCleanupManager</span>)</div><div class="line">[<span class="number">2017</span>-<span class="number">03</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">33</span>:<span class="number">43</span>,<span class="number">889</span>] INFO autopurge<span class="selector-class">.purgeInterval</span> set to <span class="number">0</span> (org<span class="selector-class">.apache</span><span class="selector-class">.zookeeper</span><span class="selector-class">.server</span><span class="selector-class">.DatadirCleanupManager</span>)</div><div class="line">[<span class="number">2017</span>-<span class="number">03</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">33</span>:<span class="number">43</span>,<span class="number">889</span>] INFO Purge task is not scheduled. (org<span class="selector-class">.apache</span><span class="selector-class">.zookeeper</span><span class="selector-class">.server</span><span class="selector-class">.DatadirCleanupManager</span>)</div><div class="line">[<span class="number">2017</span>-<span class="number">03</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">33</span>:<span class="number">43</span>,<span class="number">889</span>] WARN Either no config or no quorum defined <span class="keyword">in</span> config, running  <span class="keyword">in</span> standalone mode (org<span class="selector-class">.apache</span><span class="selector-class">.zookeeper</span><span class="selector-class">.server</span><span class="selector-class">.quorum</span><span class="selector-class">.QuorumPeerMain</span>)</div><div class="line">[<span class="number">2017</span>-<span class="number">03</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">33</span>:<span class="number">43</span>,<span class="number">920</span>] INFO Reading configuration from: config/zookeeper<span class="selector-class">.properties</span> (org<span class="selector-class">.apache</span><span class="selector-class">.zookeeper</span><span class="selector-class">.server</span><span class="selector-class">.quorum</span><span class="selector-class">.QuorumPeerConfig</span>)</div><div class="line">[<span class="number">2017</span>-<span class="number">03</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">33</span>:<span class="number">43</span>,<span class="number">920</span>] INFO Starting server (org<span class="selector-class">.apache</span><span class="selector-class">.zookeeper</span><span class="selector-class">.server</span><span class="selector-class">.ZooKeeperServerMain</span>)</div><div class="line">[<span class="number">2017</span>-<span class="number">03</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">33</span>:<span class="number">43</span>,<span class="number">943</span>] INFO Server environment:zookeeper.version=<span class="number">3.4</span>.<span class="number">9</span>-<span class="number">1757313</span>, built on <span class="number">08</span>/<span class="number">23</span>/<span class="number">2016</span> <span class="number">06</span>:<span class="number">50</span> GMT (org<span class="selector-class">.apache</span><span class="selector-class">.zookeeper</span><span class="selector-class">.server</span><span class="selector-class">.ZooKeeperServer</span>)</div><div class="line">. . . . . .</div></pre></td></tr></table></figure></p>
<p>启动 ZooKeeper 时，为脚本传入的参数是配置文件的路径。该配置文件的主要内容如下：<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># the directory where the snapshot is stored.</span></div><div class="line"><span class="attr">dataDir</span>=/tmp/zookeeper-<span class="number">0.10</span>.<span class="number">2.0</span></div><div class="line"><span class="comment"># the port at which the clients will connect</span></div><div class="line"><span class="attr">clientPort</span>=<span class="number">2181</span></div><div class="line"><span class="comment"># disable the per-ip limit on the number of connections since this is a non-production config</span></div><div class="line"><span class="attr">maxClientCnxns</span>=<span class="number">0</span></div></pre></td></tr></table></figure></p>
<p><code>dataDir</code> 选项用于配置 ZooKeeper 数据的保存路径。 <code>clientPort</code> 用于配置 ZooKeeper 监听的端口，默认为 2181。而 <code>maxClientCnxns</code> 则用以配置客户端与 ZooKeeper 之间的最大连接数。通常情况下，在一台机器上，我们只需运行一个 ZooKeeper 实例，因而 <code>dataDir</code> 和 <code>clientPort</code> 选项采用默认值即可。然而，如果我们需要在同一台主机上运行多个 ZooKeeper 实例，比如为了方便开发测试，我们需要为 Kafka 0.8.2 运行一个 ZooKeeper 实例，同时又需要为 Kafka 0.9.0 运行一个 ZooKeeper 实例，则需要注意配置这两个选项，以使它们监听的端口和保存数据文件的路径不会发生冲突。</p>
<p>现在可以启动 Kafka 服务器了：<br><figure class="highlight nix"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">$ bin/kafka-server-start.sh config/server.properties</div><div class="line">[<span class="number">2017</span>-<span class="number">03</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">35</span>:<span class="number">21</span>,<span class="number">291</span>] INFO KafkaConfig values: </div><div class="line">	advertised.host.<span class="attr">name</span> = <span class="literal">null</span></div><div class="line">	advertised.<span class="attr">listeners</span> = <span class="literal">null</span></div><div class="line">	advertised.<span class="attr">port</span> = <span class="literal">null</span></div><div class="line">	authorizer.class.<span class="attr">name</span> = </div><div class="line">	auto.create.topics.<span class="attr">enable</span> = <span class="literal">true</span></div><div class="line">	auto.leader.rebalance.<span class="attr">enable</span> = <span class="literal">true</span></div><div class="line">. . . . . .</div></pre></td></tr></table></figure></p>
<p>启动 Kafka 的脚本，执行时传入的参数同样是配置文件的路径。Kafka 的配置文件的内容比较多：<br><figure class="highlight coffeescript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># The id of the broker. This must be set to a unique integer for each broker.</span></div><div class="line">broker.id=<span class="number">0</span></div><div class="line"></div><div class="line"><span class="comment"># Switch to enable topic deletion or not, default value is false</span></div><div class="line"><span class="comment">#delete.topic.enable=true</span></div><div class="line"></div><div class="line"><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">##### Socket Server Settings ###</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">##</span></div><div class="line"></div><div class="line"><span class="comment"># The address the socket server listens on. It will get the value returned from </span></div><div class="line"><span class="comment"># java.net.InetAddress.getCanonicalHostName() if not configured.</span></div><div class="line"><span class="comment">#   FORMAT:</span></div><div class="line"><span class="comment">#     listeners = listener_name://host_name:port</span></div><div class="line"><span class="comment">#   EXAMPLE:</span></div><div class="line"><span class="comment">#     listeners = PLAINTEXT://your.host.name:9092</span></div><div class="line">listeners=PLAINTEXT:<span class="regexp">//</span>:<span class="number">9095</span></div><div class="line"></div><div class="line"><span class="comment"># Hostname and port the broker will advertise to producers and consumers. If not set, </span></div><div class="line"><span class="comment"># it uses the value for "listeners" if configured.  Otherwise, it will use the value</span></div><div class="line"><span class="comment"># returned from java.net.InetAddress.getCanonicalHostName().</span></div><div class="line"><span class="comment">#advertised.listeners=PLAINTEXT://your.host.name:9092</span></div><div class="line"></div><div class="line"><span class="comment"># Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details</span></div><div class="line"><span class="comment">#listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL</span></div><div class="line"></div><div class="line"><span class="comment"># The number of threads handling network requests</span></div><div class="line">num.network.threads=<span class="number">3</span></div><div class="line"></div><div class="line"><span class="comment"># The number of threads doing disk I/O</span></div><div class="line">num.io.threads=<span class="number">8</span></div><div class="line">. . . . . .</div><div class="line"><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">##### Log Basics ###</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">##</span></div><div class="line"></div><div class="line"><span class="comment"># A comma seperated list of directories under which to store log files</span></div><div class="line">log.dirs=/tmp/kafka-logs<span class="number">-0.10</span><span class="number">.2</span><span class="number">.0</span></div><div class="line">. . . . . .</div><div class="line"><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">##### Zookeeper ###</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">##</span></div><div class="line"></div><div class="line"><span class="comment"># Zookeeper connection string (see zookeeper docs for details).</span></div><div class="line"><span class="comment"># This is a comma separated host:port pairs, each corresponding to a zk</span></div><div class="line"><span class="comment"># server. e.g. "127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002".</span></div><div class="line"><span class="comment"># You can also append an optional chroot string to the urls to specify the</span></div><div class="line"><span class="comment"># root directory for all kafka znodes.</span></div><div class="line">zookeeper.connect=localhost:<span class="number">2185</span></div><div class="line"></div><div class="line"><span class="comment"># Timeout in ms for connecting to zookeeper</span></div><div class="line">zookeeper.connection.timeout.ms=<span class="number">6000</span></div></pre></td></tr></table></figure></p>
<p>我们可以配置 Kafka 实例的 Broker ID（<code>broker.id</code>），监听的端口号（<code>listeners=PLAINTEXT://:9095</code>），日志文件的保存路径（<code>log.dirs</code>），以及 ZooKeeper 监听的端口号（<code>zookeeper.connect</code>）等等。在建立 Kafka 集群时，需要注意不同 Kafka 实例间 <code>broker.id</code> 的值不能重复。在单主机上运行多个 Kafka 实例时，需要注意 <code>log.dirs</code> 和 <code>listeners=PLAINTEXT://:9095</code> 两个选项的值不能相同，以避免发生冲突。而在对 ZooKeeper 的运行做了配置时，比如修改了 ZooKeeper 监听的端口号，则需要有针对性地配置 <code>zookeeper.connect</code>。</p>
<h1 id="第-3-步：创建一个-topic"><a href="#第-3-步：创建一个-topic" class="headerlink" title="第 3 步：创建一个 topic"></a>第 3 步：创建一个 topic</h1><p>让我们创建一个名为 “test” 且只有一个分区和一个副本的 topic：<br><figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="comment">$</span> <span class="comment">bin/kafka</span><span class="literal">-</span><span class="comment">topics</span><span class="string">.</span><span class="comment">sh</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">create</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">zookeeper</span> <span class="comment">localhost:2181</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">replication</span><span class="literal">-</span><span class="comment">factor</span> <span class="comment">1</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">partitions</span> <span class="comment">1</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">topic</span> <span class="comment">test</span></div></pre></td></tr></table></figure></p>
<p><code>--zookeeper</code> 参数用以指定 ZooKeeper 的地址。ZooKeeper 默认监听 2181，若做了修改，则这里同样要有针对性的做修改。</p>
<p>现在如果我们运行 <code>list topic</code> 命令的话，就可以看到那个 topic 了：<br><figure class="highlight stata"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ bin/kafka-topics.<span class="keyword">sh</span> --<span class="keyword">list</span> --zookeeper localhost:2181</div><div class="line"><span class="keyword">test</span></div></pre></td></tr></table></figure></p>
<p>同样注意 ZooKeeper 的地址。</p>
<p>此外，除了手动创建 topic 外，你还可以配置你的 brokers 在发布的目标 topic 不存在时自动创建 topics。</p>
<h1 id="第-4-步：发送一些消息"><a href="#第-4-步：发送一些消息" class="headerlink" title="第 4 步：发送一些消息"></a>第 4 步：发送一些消息</h1><p>Kafka 中带有一个命令行客户端，它可以从文件或标准输入获取输入，并把它作为消息发送给 Kafka 集群。默认情况下，每行都将作为一条分开的消息。</p>
<p>运行生产者，向终端输入一些消息并发送给服务器。<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ bin/kafka-console-producer.<span class="keyword">sh</span> --broker-<span class="keyword">list</span> localhos<span class="variable">t:9092</span> --topic test</div><div class="line">This <span class="keyword">is</span> <span class="keyword">a</span> message</div><div class="line">This <span class="keyword">is</span> another message</div></pre></td></tr></table></figure></p>
<p><code>--broker-list</code> 用于指定 Kafka brokers 的地址。若修改了 Kafka 运行的配置，以非默认端口 <code>9092</code> 运行的话，则要做相应的修改。下面遇到的<code>--broker-list</code> 类似。</p>
<h1 id="第-5-步：启动消费者"><a href="#第-5-步：启动消费者" class="headerlink" title="第 5 步：启动消费者"></a>第 5 步：启动消费者</h1><p>Kafka 还有一个命令行消费者，它将把消息显示在标准输出中。<br><figure class="highlight delphi"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ bin/kafka-console-consumer.sh --bootstrap-server localhost:<span class="number">9092</span> --topic test --from-beginning</div><div class="line">This <span class="keyword">is</span> a <span class="keyword">message</span></div><div class="line">This <span class="keyword">is</span> another <span class="keyword">message</span></div></pre></td></tr></table></figure></p>
<p><code>--bootstrap-server</code> 用于指定 Kafka brokers 的地址。若修改了 Kafka 运行的配置，以非默认端口 <code>9092</code> 运行的话，则要做相应的修改。<code>--from-beginning</code> 表示，我们想要接收 Kafka 中保存的所有消息。我们还可以通过 <code>--offset</code> 参数表明我们只想接收从某个消息开始之后的消息，如：<br><figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="comment">$</span> <span class="comment">bin/kafka</span><span class="literal">-</span><span class="comment">console</span><span class="literal">-</span><span class="comment">consumer</span><span class="string">.</span><span class="comment">sh</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">bootstrap</span><span class="literal">-</span><span class="comment">server</span> <span class="comment">localhost:9095</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">topic</span> <span class="comment">test</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">partition</span> <span class="comment">0</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">offset</span> <span class="comment">1</span></div></pre></td></tr></table></figure></p>
<p>在指定了 <code>--offset</code> 参数的同时，也要指定 <code>--partition</code> 参数。</p>
<p>如果你是在不同终端中运行上述命令的，则你应该可以在生产者的终端键入消息，并在消费者终端中看到它们。</p>
<p>所有的命令行工具都具有其它的选项；不传入参数运行它们的话，将显示关于它们更详细的用法信息。</p>
<h1 id="第-6-步：搭建多broker-集群"><a href="#第-6-步：搭建多broker-集群" class="headerlink" title="第 6 步：搭建多broker 集群"></a>第 6 步：搭建多broker 集群</h1><p>目前为止我们已经运行了单个 broker，但这不好玩。对于 Kafka 而言，单个 broker 只是大小为 1 的集群，因而启动更多 broker 实例也无需太多改动。但只是为了对它有更多的认识，我们扩展我们的集群为三个节点（依然在我们的本地机器上）。</p>
<p>首先我们要为每个 broker 创建一个配置文件（在 Windows 上使用 <code>copy</code> 命令替代）：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ cp config/server<span class="selector-class">.properties</span> config/server-<span class="number">1</span><span class="selector-class">.properties</span></div><div class="line">$ cp config/server<span class="selector-class">.properties</span> config/server-<span class="number">2</span>.properties</div></pre></td></tr></table></figure></p>
<p>现在让我们编辑这些新文件并设置如下属性：<br><figure class="highlight groovy"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">config/server<span class="number">-1.</span><span class="string">properties:</span></div><div class="line">    broker.id=<span class="number">1</span></div><div class="line">    listeners=<span class="string">PLAINTEXT:</span><span class="comment">//:9093</span></div><div class="line">    log.dirs=<span class="regexp">/tmp/</span>kafka-logs<span class="number">-0.10</span><span class="number">.2</span><span class="number">.0</span><span class="number">-1</span></div><div class="line"></div><div class="line">config/server<span class="number">-2.</span><span class="string">properties:</span></div><div class="line">    broker.id=<span class="number">2</span></div><div class="line">    listeners=<span class="string">PLAINTEXT:</span><span class="comment">//:9094</span></div><div class="line">    log.dirs=<span class="regexp">/tmp/</span>kafka-logs<span class="number">-0.10</span><span class="number">.2</span><span class="number">.0</span><span class="number">-2</span></div></pre></td></tr></table></figure></p>
<p><code>broker.id</code> 属性是集群中每个节点唯一的且永久的名字。我们不得不覆写端口和日志目录只是因为我们在相同的主机上运行所有的这些 Kafka 实例，且我们要使 brokers 不试图在相同的端口上注册或相互之间覆写其它 broker 的数据。</p>
<p>我们已经有了 ZooKeeper，且我们的单节点已经启动，因而我们只需启动两个新节点：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ bin/kafka-server-start<span class="selector-class">.sh</span> config/server-<span class="number">1</span><span class="selector-class">.properties</span> &amp;</div><div class="line">. . . . . .</div><div class="line">$ bin/kafka-server-start<span class="selector-class">.sh</span> config/server-<span class="number">2</span><span class="selector-class">.properties</span> &amp;</div><div class="line">. . . . . .</div></pre></td></tr></table></figure></p>
<p>现在让我们创建一个副本因子为三的 topic：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ bin/kafka-topics<span class="selector-class">.sh</span> --create --zookeeper localhost:<span class="number">2185</span> --replication-factor <span class="number">3</span> --partitions <span class="number">1</span> --topic my-replicated-topic</div><div class="line">Created topic <span class="string">"my-replicated-topic"</span>.</div><div class="line">[<span class="number">2017</span>-<span class="number">03</span>-<span class="number">10</span> <span class="number">12</span>:<span class="number">18</span>:<span class="number">00</span>,<span class="number">737</span>] INFO [ReplicaFetcherManager on broker <span class="number">2</span>] Removed fetcher <span class="keyword">for</span> partitions my-replicated-topic-<span class="number">0</span> (kafka<span class="selector-class">.server</span><span class="selector-class">.ReplicaFetcherManager</span>)</div><div class="line">. . . . . .</div></pre></td></tr></table></figure></p>
<p>好了，现在我们有了一个集群，但我们要如何知道哪个 broker 在做什么呢？运行 “describe topics” 命令来查看：<br><figure class="highlight groovy"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ bin/kafka-topics.sh --describe --zookeeper <span class="string">localhost:</span><span class="number">2181</span> --topic my-replicated-topic</div><div class="line"><span class="string">Topic:</span>my-replicated-topic	<span class="string">PartitionCount:</span><span class="number">1</span>	<span class="string">ReplicationFactor:</span><span class="number">3</span>	<span class="string">Configs:</span></div><div class="line"><span class="symbol">	Topic:</span> my-replicated-topic	<span class="string">Partition:</span> <span class="number">0</span>	<span class="string">Leader:</span> <span class="number">2</span>	<span class="string">Replicas:</span> <span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>	<span class="string">Isr:</span> <span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span></div></pre></td></tr></table></figure></p>
<p>下面是对输出的解释。第一行给出了所有分区的总结，每个额外的行给出了关于一个分区的信息。由于我们的这个 topic 只有一个分区，因而只有一行。</p>
<ul>
<li>“leader” 是负责对给定的分区进行所有的读和写的节点。每个节点将是分区的随机选择部分的 leader。</li>
<li>“replicas” 是复制此分区的日志的节点的列表，无论它们是否为 leader，或者即使它们当前处于活动状态。</li>
<li>“isr” 是 “in-sync” 副本的集合。这是副本列表的当前处于活跃状态且被 leader 抓住了的子集。</li>
</ul>
<p>注意，在我的例子中节点 2 是 topic 仅有的分区 leader。</p>
<p>我们可以在我们最初创建的 topic 之上运行相同的命令来查看它在哪儿：<br><figure class="highlight groovy"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ bin/kafka-topics.sh --describe --zookeeper <span class="string">localhost:</span><span class="number">2181</span> --topic test</div><div class="line"><span class="string">Topic:</span>test	<span class="string">PartitionCount:</span><span class="number">1</span>	<span class="string">ReplicationFactor:</span><span class="number">1</span>	<span class="string">Configs:</span></div><div class="line"><span class="symbol">	Topic:</span> test	<span class="string">Partition:</span> <span class="number">0</span>	<span class="string">Leader:</span> <span class="number">0</span>	<span class="string">Replicas:</span> <span class="number">0</span>	<span class="string">Isr:</span> <span class="number">0</span></div></pre></td></tr></table></figure></p>
<p>没什么值得惊讶的地方 —— 最初的 topic 没有副本，且在服务器 0 上，创建时我们的集群中仅有的服务器。</p>
<p>让我们给我们的新 topic 发布一些消息：<br><figure class="highlight stata"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$ bin/kafka-console-producer.<span class="keyword">sh</span> --broker-<span class="keyword">list</span> localhost:9092 --topic my-replicated-topic</div><div class="line">. . . . . .</div><div class="line">my <span class="keyword">test</span> message 1</div><div class="line">my <span class="keyword">test</span> message 2</div><div class="line">^C</div></pre></td></tr></table></figure></p>
<p>现在让我们消费这些消息：<br><figure class="highlight stata"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$ bin/kafka-console-consumer.<span class="keyword">sh</span> --<span class="keyword">bootstrap</span>-server localhost:9092 --from-beginning --topic my-replicated-topic</div><div class="line">. . . . . .</div><div class="line">my <span class="keyword">test</span> message 1</div><div class="line">my <span class="keyword">test</span> message 2</div><div class="line">^C</div></pre></td></tr></table></figure></p>
<p>现在让我们测试一下容错。Broker 2 扮演 leader 的角色，让我们杀掉它：<br><figure class="highlight crystal"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ ps aux | grep server-<span class="number">2</span>.properties</div><div class="line">hanpfei+  <span class="number">1808</span>  <span class="number">0.0</span>  <span class="number">0.0</span>  <span class="number">24608</span>  <span class="number">1040</span> pts/<span class="number">27</span>   S+   <span class="number">13</span>:<span class="number">45</span>   <span class="number">0</span>:<span class="number">00</span> grep --color=auto server-<span class="number">2</span>.properties</div><div class="line">hanpfei+ <span class="number">30471</span>  <span class="number">0.7</span>  <span class="number">3.0</span> <span class="number">6764728</span> <span class="number">498624</span> pts/<span class="number">27</span> Sl   <span class="number">12</span>:<span class="number">16</span>   <span class="number">0</span>:<span class="number">38</span> /usr/<span class="class"><span class="keyword">lib</span>/<span class="title">jvm</span>/<span class="title">java</span>-1.8.0-<span class="title">openjdk</span>-<span class="title">amd64</span>/<span class="title">bin</span>/<span class="title">java</span> . . .</span></div><div class="line">$ kill -<span class="number">9</span> <span class="number">30471</span></div></pre></td></tr></table></figure></p>
<p>在 Windows 上使用：<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ wmic process <span class="built_in">get</span> processid,caption,commandline | <span class="keyword">find</span> <span class="string">"java.exe"</span> | <span class="keyword">find</span> <span class="string">"server-2.properties"</span></div><div class="line">java.<span class="keyword">exe</span>    java  -Xmx1G -Xms1G -server -XX:+UseG1GC ... build\libs\kafka_2.<span class="number">10</span>-<span class="number">0.10</span>.<span class="number">2.0</span>.jar<span class="comment">"  kafka.Kafka config\server-2.properties    644</span></div><div class="line">$ taskkill /pid <span class="number">644</span> /<span class="keyword">f</span></div></pre></td></tr></table></figure></p>
<p>Leadership 已经切换为了副 brokers 中的一个，且节点 2 不再在 in-sync 副本集了：<br><figure class="highlight groovy"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ bin/kafka-topics.sh --describe --zookeeper <span class="string">localhost:</span><span class="number">2181</span> --topic my-replicated-topic</div><div class="line"><span class="string">Topic:</span>my-replicated-topic	<span class="string">PartitionCount:</span><span class="number">1</span>	<span class="string">ReplicationFactor:</span><span class="number">3</span>	<span class="string">Configs:</span></div><div class="line"><span class="symbol">	Topic:</span> my-replicated-topic	<span class="string">Partition:</span> <span class="number">0</span>	<span class="string">Leader:</span> <span class="number">0</span>	<span class="string">Replicas:</span> <span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>	<span class="string">Isr:</span> <span class="number">0</span>,<span class="number">1</span></div></pre></td></tr></table></figure></p>
<p>但消息依然可以用于消费，即使最初接管写操作的 leader 不在了：<br><figure class="highlight stata"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$ bin/kafka-console-consumer.<span class="keyword">sh</span> --<span class="keyword">bootstrap</span>-server localhost:9092 --from-beginning --topic my-replicated-topic</div><div class="line">...</div><div class="line">my <span class="keyword">test</span> message 1</div><div class="line">my <span class="keyword">test</span> message 2</div><div class="line">^C</div></pre></td></tr></table></figure></p>
<p>Kafka broker 实例是如何被组织为容错好，可伸缩性好的 brokers 集群的呢？由上面创建 Kafka 集群的过程，不难猜测，是由 ZooKeeper 帮忙将 Kafka 实例节点组织为集群的。</p>
<h1 id="第-7-步：使用-Kafka-连接来导入-导出数据"><a href="#第-7-步：使用-Kafka-连接来导入-导出数据" class="headerlink" title="第 7 步：使用 Kafka 连接来导入/导出数据"></a>第 7 步：使用 Kafka 连接来导入/导出数据</h1><p>初学 Kafka，从终端读取数据并把它写回终端是比较方便的，但你可能想要使用其它来源的数据，或从 Kafka 导出数据到其它系统。对于许多系统而言，无需编写定制的集成代码，你可以使用 Kafka Connect 导入或导出数据。</p>
<p>Kafka Connect 是 Kafka 中包含的工具，它可用于从 Kafka 导出数据或向 Kafka 导入数据。它是一个运行 <em>connectors</em> 的可扩展的工具，其实现了与外部系统交互的定制逻辑。在这份入门文档中，我们将看到如何以将文件中的数据导入到 Kafka topic 并将 Kafka topic 的数据导出到文件的简单的 connectors 运行 Kafka Connect</p>
<p>首先我们从创建一些种子数据用以测试开始：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">echo</span> <span class="_">-e</span> <span class="string">"foo\nbar"</span> &gt; test.txt</div></pre></td></tr></table></figure></p>
<p>接下来，我们将启动两个 connectors，以 <em>standalone</em>  模式运行，这意味着它们运行于单独的本地的专门的进程中。我们提供三个配置文件作为参数。第一个总是 Kafka Connect 进程的配置，包含诸如要连接的 Kafka broker 和数据的序列化格式等通用的配置。其余的配置文件每个都描述一个要创建的 connector。这些文件包含一个唯一的 connector 名字，要初始化的 connector 类，及 connector 需要的其它配置。<br><figure class="highlight arduino"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ bin/<span class="built_in">connect</span>-standalone.sh <span class="built_in">config</span>/<span class="built_in">connect</span>-standalone.properties <span class="built_in">config</span>/<span class="built_in">connect</span>-file-source.properties <span class="built_in">config</span>/<span class="built_in">connect</span>-file-sink.properties</div></pre></td></tr></table></figure></p>
<p>这些示例配置文件，包含在 Kafka 中，使用你之前启动的默认的本地集群配置并创建两个 connectors：第一个是从输入文件中读取行的源 connector，并为 Kafka topic 每行生产一条消息，第二个是从 Kafka topic 读取消息并为每个消息生产一个输出行的输出 connector。</p>
<p><code>config/connect-standalone.properties</code> 配置文件的主要内容如下：<br><figure class="highlight vhdl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"># These are defaults. This <span class="keyword">file</span> just demonstrates how <span class="keyword">to</span> override some settings.</div><div class="line">bootstrap.servers=localhost:<span class="number">9095</span></div><div class="line"></div><div class="line"># The converters specify the format <span class="keyword">of</span> data <span class="keyword">in</span> Kafka <span class="keyword">and</span> how <span class="keyword">to</span> translate it into Connect data. Every Connect user will</div><div class="line"># need <span class="keyword">to</span> configure these based <span class="keyword">on</span> the format they want their data <span class="keyword">in</span> <span class="keyword">when</span> loaded from <span class="keyword">or</span> stored into Kafka</div><div class="line">key.converter=org.apache.kafka.connect.json.JsonConverter</div><div class="line">value.converter=org.apache.kafka.connect.json.JsonConverter</div><div class="line"># Converter-specific settings can be passed <span class="keyword">in</span> by prefixing the Converter<span class="symbol">'s</span> setting <span class="keyword">with</span> the converter we want <span class="keyword">to</span> apply</div><div class="line"># it <span class="keyword">to</span></div><div class="line">key.converter.schemas.enable=<span class="literal">true</span></div><div class="line">value.converter.schemas.enable=<span class="literal">true</span></div><div class="line"></div><div class="line"># The internal converter used <span class="keyword">for</span> offsets <span class="keyword">and</span> config data <span class="keyword">is</span> configurable <span class="keyword">and</span> must be specified, but most users will</div><div class="line"># always want <span class="keyword">to</span> <span class="keyword">use</span> the built-<span class="keyword">in</span> <span class="keyword">default</span>. Offset <span class="keyword">and</span> config data <span class="keyword">is</span> never visible outside <span class="keyword">of</span> Kafka Connect <span class="keyword">in</span> this format.</div><div class="line">internal.key.converter=org.apache.kafka.connect.json.JsonConverter</div><div class="line">internal.value.converter=org.apache.kafka.connect.json.JsonConverter</div><div class="line">internal.key.converter.schemas.enable=<span class="literal">false</span></div><div class="line">internal.value.converter.schemas.enable=<span class="literal">false</span></div><div class="line"></div><div class="line">offset.storage.<span class="keyword">file</span>.filename=/tmp/connect.offsets</div><div class="line"># Flush much faster than normal, which <span class="keyword">is</span> useful <span class="keyword">for</span> testing/debugging</div><div class="line">offset.flush.interval.ms=<span class="number">10000</span></div></pre></td></tr></table></figure></p>
<p>同样注意，在修改了 Kafka broker默认的监听端口时，要相应地修改这里的 <code>bootstrap.servers</code> 选项的值。</p>
<p>而 <code>config/connect-file-source.properties</code> 的主要内容如下：<br><figure class="highlight stata"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">name=<span class="keyword">local</span>-<span class="keyword">file</span>-source</div><div class="line">connector.<span class="keyword">class</span>=FileStreamSource</div><div class="line">tasks.max=1</div><div class="line"><span class="keyword">file</span>=<span class="keyword">test</span>.txt</div><div class="line">topic=connect-<span class="keyword">test</span></div></pre></td></tr></table></figure></p>
<p>可以通过这个文件配置 connector 的类，输入文件的路径，和 kafka topic。<code>config/connect-file-sink.properties</code> 的内容类似：<br><figure class="highlight stata"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">name=<span class="keyword">local</span>-<span class="keyword">file</span>-sink</div><div class="line">connector.<span class="keyword">class</span>=FileStreamSink</div><div class="line">tasks.max=1</div><div class="line"><span class="keyword">file</span>=<span class="keyword">test</span>.sink.txt</div><div class="line">topics=connect-<span class="keyword">test</span></div></pre></td></tr></table></figure></p>
<p>可以在这里配置 输出文件的路径等。</p>
<p>在启动期间你将看到大量的日志消息，包括一些表明 connectors 正在被初始化的。一旦 Kafka Connect 进程启动了，源 connector 应该开始从 <code>test.txt</code> 读取行并将它们生产到 <code>connect-test</code> topic，而输出 connector 应该开始从 <code>connect-test</code> topic 读取消息并将它们写入文件 <code>test.sink.txt</code>。我们可以通过检查输出文件的内容验证数据已经通过整个管道被传送了。<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ cat test<span class="selector-class">.sink</span><span class="selector-class">.txt</span></div><div class="line">foo</div><div class="line">bar</div></pre></td></tr></table></figure></p>
<p>注意，数据被存储在了 Kafka topic <code>connect-test</code>，因而我们也可以运行终端消费者查看 topic 中的数据（或使用定制的消费者代码处理它）：<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ bin/kafka-<span class="built_in">console</span>-consumer.sh --bootstrap-server localhost:<span class="number">9092</span> --topic connect-test --<span class="keyword">from</span>-beginning</div><div class="line">&#123;<span class="string">"schema"</span>:&#123;<span class="string">"type"</span>:<span class="string">"string"</span>,<span class="string">"optional"</span>:<span class="literal">false</span>&#125;,<span class="string">"payload"</span>:<span class="string">"foo"</span>&#125;</div><div class="line">&#123;<span class="string">"schema"</span>:&#123;<span class="string">"type"</span>:<span class="string">"string"</span>,<span class="string">"optional"</span>:<span class="literal">false</span>&#125;,<span class="string">"payload"</span>:<span class="string">"bar"</span>&#125;</div></pre></td></tr></table></figure></p>
<p>Connectors 持续处理数据，因而我们可以给文件添加数据，并看到它通过了管道：<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ echo <span class="string">"Another line"</span> <span class="meta">&gt;&gt; </span>test.txt</div></pre></td></tr></table></figure></p>
<p>你应该在终端消费者输出和输出文件中看到了相应的行。</p>
<h1 id="第-8-步：使用-Kafka-Streams-处理数据"><a href="#第-8-步：使用-Kafka-Streams-处理数据" class="headerlink" title="第 8 步：使用 Kafka Streams 处理数据"></a>第 8 步：使用 Kafka Streams 处理数据</h1><p>Kafka Streams 是一个 Kafka 客户端库，用于实时流处理和分析存储在 Kafka brokers 中的数据。这份入门文档示例将演示如何以这个库运行一个流应用程序。这里是 <code>[WordCountDemo](https://github.com/apache/kafka/blob/%7BdotVersion%7D/streams/examples/src/main/java/org/apache/kafka/streams/examples/wordcount/WordCountDemo.java)</code> 示例代码的要旨（转换为了使用 Java 8 lambda 表达式以方便阅读）：<br><figure class="highlight processing"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Serializers/deserializers (serde) for String and Long types</span></div><div class="line"><span class="keyword">final</span> Serde&lt;<span class="keyword">String</span>&gt; stringSerde = Serdes.<span class="keyword">String</span>();</div><div class="line"><span class="keyword">final</span> Serde&lt;Long&gt; longSerde = Serdes.Long();</div><div class="line"></div><div class="line"><span class="comment">// Construct a `KStream` from the input topic ""streams-file-input", where message values</span></div><div class="line"><span class="comment">// represent lines of text (for the sake of this example, we ignore whatever may be stored</span></div><div class="line"><span class="comment">// in the message keys).</span></div><div class="line">KStream&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt; textLines = builder.stream(stringSerde, stringSerde, <span class="string">"streams-file-input"</span>);</div><div class="line"></div><div class="line">KTable&lt;<span class="keyword">String</span>, Long&gt; wordCounts = textLines</div><div class="line">    <span class="comment">// Split each text line, by whitespace, into words.</span></div><div class="line">    .flatMapValues(value -&gt; Arrays.asList(value.toLowerCase().<span class="built_in">split</span>(<span class="string">"\\W+"</span>)))</div><div class="line"></div><div class="line">    <span class="comment">// Group the text words as message keys</span></div><div class="line">    .groupBy((<span class="built_in">key</span>, value) -&gt; value)</div><div class="line"></div><div class="line">    <span class="comment">// Count the occurrences of each word (message key).</span></div><div class="line">    .count(<span class="string">"Counts"</span>)</div><div class="line"></div><div class="line"><span class="comment">// Store the running counts as a changelog stream to the output topic.</span></div><div class="line">wordCounts.to(stringSerde, longSerde, <span class="string">"streams-wordcount-output"</span>);</div></pre></td></tr></table></figure></p>
<p>它实现了 WordCount 算法，其从输入文本计算单词出现直方图。然而，不像其它你之前可能已经见过的操作有界数据的 WordCount 例子，这个 WordCount 示例应用的行为有一点点不同，因为它被设计来操作数据的 <strong>无限的，无界的流</strong>。类似于有界的变体，它是一个跟踪并更新单词个数的有状态算法。然而，由于它必须假设数据为潜在无界的，它将在持续处理更多数据时周期性地输出它的当前状态和结果，因为它无法知道它何时处理完“所有”输入数据。</p>
<p>第一步，我们将为 Kafka topic 准备输入数据，其将会在后面被 Kafka Streams 应用处理掉。<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ <span class="keyword">echo</span> -<span class="keyword">e</span> <span class="string">"all streams lead to kafka\nhello kafka streams\njoin kafka summit"</span> &gt; <span class="keyword">file</span>-<span class="built_in">input</span>.txt</div></pre></td></tr></table></figure></p>
<p>或在 Windows 上：<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ <span class="keyword">echo</span> <span class="keyword">all</span> streams lead <span class="keyword">to</span> kafka&gt; <span class="keyword">file</span>-<span class="built_in">input</span>.txt</div><div class="line">$ <span class="keyword">echo</span> hello kafka streams&gt;&gt; <span class="keyword">file</span>-<span class="built_in">input</span>.txt</div><div class="line">$ <span class="keyword">echo</span>|<span class="keyword">set</span> /<span class="keyword">p</span>=<span class="keyword">join</span> kafka summit&gt;&gt; <span class="keyword">file</span>-<span class="built_in">input</span>.txt</div></pre></td></tr></table></figure></p>
<p>接下来，我们使用终端生产者将这些输入数据发送到名为 <strong>streams-file-input</strong> 的输入 topic 。终端生产者一行接一行地从 STDIN 读取数据，并将每一行作为一条分开的以 null 为 key 且 value 被编码为字符串的 Kafka 消息发送给 topic（实践上，流数据将可能持续地流入应用程序启动并运行的 Kafka）：<br><figure class="highlight haml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$ bin/kafka-topics.sh --create \</div><div class="line">            -<span class="ruby">-zookeeper <span class="symbol">localhost:</span><span class="number">2181</span> \</span></div><div class="line">            -<span class="ruby">-replication-factor <span class="number">1</span> \</span></div><div class="line">            -<span class="ruby">-partitions <span class="number">1</span> \</span></div><div class="line">            -<span class="ruby">-topic streams-file-input</span></div></pre></td></tr></table></figure></p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ bin/kafka-console-producer.<span class="keyword">sh</span> --broker-<span class="keyword">list</span> localhos<span class="variable">t:9092</span> --topic streams-<span class="keyword">file</span>-<span class="built_in">input</span> &lt; <span class="keyword">file</span>-<span class="built_in">input</span>.txt</div></pre></td></tr></table></figure>
<p>我们可以运行 WordCount 示例应用来处理输入数据：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ bin/kafka-run-class<span class="selector-class">.sh</span> org<span class="selector-class">.apache</span><span class="selector-class">.kafka</span><span class="selector-class">.streams</span><span class="selector-class">.examples</span><span class="selector-class">.wordcount</span><span class="selector-class">.WordCountDemo</span></div></pre></td></tr></table></figure></p>
<p>示例应用将从输入 topic <strong>streams-file-input</strong> 读取，对每个读取的消息执行 WordCount 算法的计算，并将它的当前结果持续写入到输出 topic <strong>streams-wordcount-output</strong>。因此，除了作为结果的日志项被写回到 Kafka，将没有任何 STDOUT 输出。示例应用将运行几秒钟，然后不像典型的流处理应用程序，自动地终止。</p>
<p>我们现在可以通过读取它的输出 topic 来深入 WordCount 示例应用程序的输出：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">$ bin/kafka-console-consumer<span class="selector-class">.sh</span> --bootstrap-server localhost:<span class="number">9092</span> \</div><div class="line">            --topic streams-wordcount-output \</div><div class="line">            --from-beginning \</div><div class="line">            --formatter kafka<span class="selector-class">.tools</span><span class="selector-class">.DefaultMessageFormatter</span> \</div><div class="line">            --property print.key=true \</div><div class="line">            --property print.value=true \</div><div class="line">            --property key.deserializer=org<span class="selector-class">.apache</span><span class="selector-class">.kafka</span><span class="selector-class">.common</span><span class="selector-class">.serialization</span><span class="selector-class">.StringDeserializer</span> \</div><div class="line">            --property value.deserializer=org<span class="selector-class">.apache</span><span class="selector-class">.kafka</span><span class="selector-class">.common</span><span class="selector-class">.serialization</span><span class="selector-class">.LongDeserializer</span></div></pre></td></tr></table></figure></p>
<p>下面的输出数据被打印到终端：<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">all     <span class="number">1</span></div><div class="line">lead    <span class="number">1</span></div><div class="line">to      <span class="number">1</span></div><div class="line">hello   <span class="number">1</span></div><div class="line">streams <span class="number">2</span></div><div class="line">join    <span class="number">1</span></div><div class="line">kafka   <span class="number">3</span></div><div class="line">summit  <span class="number">1</span></div></pre></td></tr></table></figure></p>
<p>这里，第一列是 <code>java.lang.String</code> 格式的 Kafka 消息 key，第二列是 <code>java.lang.Long</code> 格式的消息值。注意，输出实际上是一个持续更新的流，其中每个数据记录（比如，上面最初的输出中的每行）是单个单词更新后的个数，比如 “kafka” 这样的 aka 记录 key。对于相同 key 的多个记录，每个后面的记录是前一个的更新。</p>
<p>下面的两个图说明了幕后发生的事情。第一列展示了计数 <code>count</code> 的单次出现的 <code>KTable&lt;String, Long&gt;</code> 的当前状态的发展。第二列展示了改变记录导致 KTable 的状态更新，且被发送到输出 Kafka topic <strong>streams-wordcount-output</strong>。<br><img src="https://www.wolfcstech.com/images/1315506-36bbd3d502fc1322.png" alt=""></p>
<p><img src="https://www.wolfcstech.com/images/1315506-1e6a10aae3383f6b.png" alt=""></p>
<p>首先，文本行 “all streams lead to kafka” 被处理。<code>KTable</code> 被创建，每个新单词导致一个新表项（由绿色背景高亮），然后一个对应的修改记录被发送给下游的 <code>KStream</code>。</p>
<p>当第二个文本行 “hello kafka streams” 被处理时，我们首次观察到，<code>KTable</code> 中已有的项被更新（这里：是单次 “kafka” 和 “streams”）。再次，修改记录被发送给输出 topic。</p>
<p>以此类推（我们跳过对第三行的处理的描述）。这解释了为什么输出 topic 具有我们上面展示的内容，因为它包含完整的修改记录。</p>
<p>超越这个具体示例的范围，Kafka Streams 在这里做的是利用表和 changelog 流之间的对偶性（这里：table = KTable，changelog 流 = 下游的 KStream）：你可以发布每个表的修改到一个流，且如果你从头至尾消费了整个 changelog 流，则你可以重建表的内容。</p>
<p>现在你可以向 <strong>streams-file-input</strong> topic 写更多输入消息，并观察到额外的消息被添加到了 <strong>streams-wordcount-output</strong> topic，反映了更新的单词计数（比如，使用终端生产者和终端消费者，像上面描述的那样）。</p>
<p>你可以通过 Ctrl-C 来停止终端消费者。</p>
<h3 id="打赏"><a href="#打赏" class="headerlink" title="打赏"></a><a href="https://www.wolfcstech.com/about/donate.html">打赏</a></h3><p><a href="https://kafka.apache.org/quickstart" target="_blank" rel="external">原文</a></p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-kafka/" target="_blank" rel="external">Apache kafka 工作原理介绍</a></p>
</div><div class="tags"><a href="/tags/后台开发/">后台开发</a><a href="/tags/Java开发/">Java开发</a></div><div class="post-nav"><a href="/2017/03/17/HBase数据模型/" class="pre">HBase数据模型</a><a href="/2017/03/09/QUIC加密协议/" class="next">QUIC加密协议</a></div><div id="disqus_thread"></div></div></div></div><div class="layout-r"><div id="sidebar"><div class="widget"><div id="search-form"><div id="result-mask" class="hide"></div><label><input id="search-key" type="text" autocomplete="off" placeholder="请输入关键字..."/></label><div id="result-wrap" class="hide"><div id="search-result"></div></div><div class="hide"><template id="search-tpl"><div class="item"><a href="/{path}" title="{title}"><div class="title">{title}</div><div class="time">{date}</div><div class="tags">{tags}</div></a></div></template></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-gui"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Android-图形系统/">Android 图形系统</a><span class="category-list-count">16</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Android开发/">Android开发</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/C-C-开发/">C/C++开发</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Java开发/">Java开发</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux内核/">Linux内核</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/live555/">live555</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/业界趣闻/">业界趣闻</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/后台开发/">后台开发</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/安全/">安全</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/网络协议/">网络协议</a><span class="category-list-count">39</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/网络调试/">网络调试</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/随想杂谈/">随想杂谈</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/音视频开发/">音视频开发</a><span class="category-list-count">3</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-biao"> 标签</i></div><div class="tagcloud"><a href="/tags/UDT/" style="font-size: 15px;">UDT</a> <a href="/tags/网络协议/" style="font-size: 15px;">网络协议</a> <a href="/tags/Android开发/" style="font-size: 15px;">Android开发</a> <a href="/tags/源码分析/" style="font-size: 15px;">源码分析</a> <a href="/tags/chromium/" style="font-size: 15px;">chromium</a> <a href="/tags/后台开发/" style="font-size: 15px;">后台开发</a> <a href="/tags/Java开发/" style="font-size: 15px;">Java开发</a> <a href="/tags/QUIC/" style="font-size: 15px;">QUIC</a> <a href="/tags/翻译/" style="font-size: 15px;">翻译</a> <a href="/tags/网络调试/" style="font-size: 15px;">网络调试</a> <a href="/tags/OpenGL/" style="font-size: 15px;">OpenGL</a> <a href="/tags/HTTP2/" style="font-size: 15px;">HTTP2</a> <a href="/tags/图形图像/" style="font-size: 15px;">图形图像</a> <a href="/tags/安全/" style="font-size: 15px;">安全</a> <a href="/tags/HTTPS/" style="font-size: 15px;">HTTPS</a> <a href="/tags/C-C-开发/" style="font-size: 15px;">C/C++开发</a> <a href="/tags/音视频开发/" style="font-size: 15px;">音视频开发</a> <a href="/tags/Linux内核/" style="font-size: 15px;">Linux内核</a> <a href="/tags/live555/" style="font-size: 15px;">live555</a> <a href="/tags/Django/" style="font-size: 15px;">Django</a> <a href="/tags/Go-语言/" style="font-size: 15px;">Go 语言</a> <a href="/tags/Linux/" style="font-size: 15px;">Linux</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-fei"> 归档</i></div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/10/">十月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">九月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">八月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">七月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">六月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">五月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">四月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">三月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">二月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">一月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/12/">十二月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">十一月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">十月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/08/">八月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/07/">七月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/04/">四月 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/09/">九月 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/07/">七月 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/09/">九月 2013</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/07/">七月 2013</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/05/">五月 2013</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/02/">二月 2013</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-xie"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2017/10/31/qcow2_on_linux/">在 Linux 上如何挂载 qcow2 磁盘镜像</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/21/android_graphics_gralloc/">Android 图形系统之gralloc</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/20/android_graphics_bufferalloc/">Android 图形系统之图形缓冲区分配</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/16/opengles_android_emulation/">Android 硬件 OpenGL ES 模拟设计概述</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/15/egl_context_creation/">EGL Context 创建</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/14/egl_init_drivers/">Android 图形驱动初始化</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/13/opengl_on_android_with_sv/">在 Android 中使用 OpenGL</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/12/android_debug_gdb/">使用 GDB 调试 Android 应用</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/11/android_emulator_dev/">Android 模拟器下载、编译及调试</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/08/live555_src_analysis_start_streaming/">live555 源码分析：播放启动</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-you"> 友情链接</i></div><ul></ul><a href="https://developer.android.com/index.html" title="Android Developers" target="_blank">Android Developers</a><ul></ul><a href="https://source.android.com/" title="Android Open Source Project" target="_blank">Android Open Source Project</a><ul></ul><a href="http://www.vants.org/" title="蚂蚁网" target="_blank">蚂蚁网</a><ul></ul><a href="http://www.tworice.com" title="亚马逊分类目录" target="_blank">亚马逊分类目录</a></div><div class="widget"><div class="widget-title"><i class="fa fa-commt"> 最近评论</i></div><script type="text/javascript" src="//wolfcstech.disqus.com/recent_comments_widget.js?num_items=5&amp;hide_avatars=1&amp;avatar_size=32&amp;excerpt_length=20&amp;hide_mods=1"></script></div></div></div></div><a id="totop" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.2.0" async></script><div id="footer"><div class="footer-info"><p><a href="/baidusitemap.xml">网站地图</a> |<a href="/atom.xml">订阅本站</a> |<span>联系博主：<a href="mailto:hanpfei@gmail.com" target="_blank" class="fa fa-email"> </a><a href="undefined" target="_blank" class="fa fa-weibo"></a><a href="https://github.com/hanpfei" target="_blank" class="fa fa-github"> </a></span></p><p><span> Copyright &copy;<a href="/." rel="nofollow">Han Pengfei</a></span><span> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a></span><span><a rel="nofollow" target="_blank" href="https://github.com/chaooo/hexo-theme-BlueLake"> Theme </a>by<a rel="nofollow" target="_blank" href="https://github.com/chaooo"> Charles.</a></span></p></div></div></div><script>var disqus_shortname = 'wolfcstech';
var disqus_identifier = '2017/03/10/Kafka入门/';
var disqus_title = 'Kafka 入门';
var disqus_url = 'https://www.wolfcstech.com/2017/03/10/Kafka入门/';
(function() {
  var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//wolfcstech.disqus.com/count.js" async></script><script type="text/javascript" src="/js/search.json.js?v=1.2.0"></script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?3c736b12c32c019fd9ff6c825b6b9b44";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();</script></body></html>